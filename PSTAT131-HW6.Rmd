---
title: "PSTAT131-HW6"
author: "Joshua Price"
date: "5/26/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo=FALSE, include = FALSE}
library(tidymodels)
library(ISLR) # For the Smarket data set
library(ISLR2) # For the Bikeshare data set
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
library(pROC)
library(ggplot2)
library(glmnet)
library(janitor)
library(randomForest)
library(xgboost)
library(vip)
library(rpart.plot)
library(ranger)
library(corrplot)
tidymodels_prefer()
```


**Exercise 1**  
Read in the data and set things up as in Homework 5:

-Use clean_names()
-Filter out the rarer Pokémon types
-Convert type_1 and legendary to factors
Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using v-fold cross-validation, with v = 5. Stratify on the outcome variable.

Set up a recipe to predict type_1 with legendary, generation, sp_atk, attack, speed, defense, hp, and sp_def:

-Dummy-code legendary and generation;
-Center and scale all predictors.

```{r}
library(janitor)
pokemon <- read.csv("Pokemon.csv")
pokemon_cn <- pokemon %>% clean_names()
set.seed(2505)

pokemon_f <- pokemon_cn %>% filter(type_1 == "Bug" | type_1 ==  "Fire" | type_1 == "Grass"| type_1 =="Normal"| type_1 =="Water"| type_1 =="Psychic")

pokemon_f$type_1 <- as.factor(pokemon_f$type_1)
pokemon_f$legendary <- as.factor(pokemon_f$legendary)
pokemon_f$generation <- as.factor(pokemon_f$generation)

pokemon_split <- initial_split(pokemon_f, prop=.7, strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata=type_1)

pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def,data=pokemon_train) %>% step_dummy(c(legendary, generation)) %>% step_scale() %>% step_center()
```



**Exercise 2**  
Create a correlation matrix of the training set, using the corrplot package. Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).

What relationships, if any, do you notice? Do these relationships make sense to you?  
-i only selected numeric between pokemon stats are numeric 
-there's a high correlation between total and everything which makes sense because total is everything added up.
-There;s also a higher correlation between sp def and def and atk with sp atk. These make sense because most pokemon specialize between one or the other.
```{r}
pokemon_cor <- pokemon_train %>% select_if(is.numeric) %>% cor(use = "complete.obs") %>% corrplot(type = "lower", diag = FALSE)
```
**Exercise 3**  
First, set up a decision tree model and workflow. Tune the cost_complexity hyperparameter. Use the same levels we used in Lab 7 – that is, range = c(-3, -1). Specify that the metric we want to optimize is roc_auc.

Print an autoplot() of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?  
-The roc auc increases a little bit as cost complexity increased but decreases a lot after its peak. It performs better with a larger complexity penalty

```{r}
tree_spec <-decision_tree() %>% set_engine("rpart")

class_tree_spec <- tree_spec %>% set_mode("classification")
  
class_tree_wf <- workflow() %>% add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>% add_recipe(pokemon_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(class_tree_wf, resamples = pokemon_folds, grid = param_grid, metrics = metric_set(roc_auc))
autoplot(tune_res)
```
**Exercise 4**  
What is the roc_auc of your best-performing pruned decision tree on the folds? Hint: Use collect_metrics() and arrange().
```{r}
tree_roc_auc <- collect_metrics(tune_res) %>% arrange(-mean)
tree_roc_auc
```

**Exercise 5**  
Using rpart.plot, fit and visualize your best-performing pruned decision tree with the training set.

```{r}
best_complexity <- select_best(tune_res)
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)

class_tree_final_fit %>% extract_fit_engine() %>% rpart.plot()
```

**Exercise 5**  
Now set up a random forest model and workflow. Use the ranger engine and set importance = "impurity". Tune mtry, trees, and min_n. Using the documentation for rand_forest(), explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that mtry should not be smaller than 1 or larger than 8. Explain why not. What type of model would mtry = 8 represent?  
-mtry is the number of variables randomly sampled at each split
-trees is the number of trees
-min_n is the minimum number of points in one group for a zplit
-We cant predict with 0 predictors and we cant predict with more predictors than we have thus it's 1-8
-mtry = 8 would be a bagging model

```{r}
forest_spec <- rand_forest() %>% set_engine("ranger", importance = "impurity") %>% set_mode("classification")

forest_wf <- workflow() %>% add_model(forest_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>% add_recipe(pokemon_recipe)

multi_param_grid <- grid_regular(mtry(range = c(1,8)), trees(range(1,200)), min_n(range(1,30)), levels = 8)

multi_tune_res <- tune_grid(forest_wf, resamples = pokemon_folds, grid = multi_param_grid, metrics = metric_set(roc_auc) )
```

**Exercise 6**  
Specify roc_auc as a metric. Tune the model and print an autoplot() of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?  
- 1 tree has low performance, 200 has highest performance but theres not much difference between 57 to 200 trees

```{r}
autoplot(multi_tune_res)
```

**Exercise 7**  
What is the roc_auc of your best-performing random forest model on the folds? Hint: Use collect_metrics() and arrange().

```{r}
collect_metrics(multi_tune_res) %>% arrange(-mean)

best_forest_model <- select_best(multi_tune_res, metric = "roc_auc")
```

**Exercise 8**  
Create a variable importance plot, using vip(), with your best-performing random forest model fit on the training set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?  
- the most useful variable were the stats of the pokemon and the least useful were the generation they're from  
```{r}
best_model_final <- finalize_workflow(forest_wf, best_forest_model)
best_model_final_fit <- fit(best_model_final, data = pokemon_train)
best_model_final_fit %>% extract_fit_engine() %>% vip()
```


**Exercise 9**  
Finally, set up a boosted tree model and workflow. Use the xgboost engine. Tune trees. Create a regular grid with 10 levels; let trees range from 10 to 2000. Specify roc_auc and again print an autoplot() of the results.

What do you observe?

What is the roc_auc of your best-performing boosted tree model on the folds? Hint: Use collect_metrics() and arrange().
- 10 trees roc_auc has the lowest value while 452 trees has the highest with .723
```{r}
boost_spec <- boost_tree() %>% set_engine("xgboost") %>% set_mode("classification")

boost_wf <- workflow() %>% add_model(boost_spec %>% set_args(trees = tune())) %>% add_recipe(pokemon_recipe)

boost_grid <- grid_regular(trees(range = c(10,2000)), levels = 10)

boost_tune_res <- tune_grid(boost_wf, resamples = pokemon_folds, grid = boost_grid, metrics = metric_set(roc_auc))

autoplot(boost_tune_res)
collect_metrics(boost_tune_res) %>% arrange(-mean)

best_boost_final <- select_best(boost_tune_res)
best_boost_final_model <- finalize_workflow(boost_wf, best_boost_final)
best_boost_final_model_fit <- fit(best_boost_final_model, data = pokemon_train)
```

**Exercise 10**  
Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use select_best(), finalize_workflow(), and fit() to fit it to the testing set.

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?  
- my model was most accurate at predicting normal, then water and bug,

```{r}
pruned_roc_auc <- collect_metrics(tune_res) %>% arrange(-mean)
forest_roc_auc <- collect_metrics(multi_tune_res) %>% arrange(-mean)
boost_roc_auc <- collect_metrics(boost_tune_res) %>% arrange(-mean)

roc_auc_means <- c(pruned_roc_auc$mean[1], forest_roc_auc$mean[1], boost_roc_auc$mean[1])

Models <- c("Pruned Tree", "Random Forest", "Boosted Tree")
Total <- tibble(roc_auc = roc_auc_means, models = Models)
Total

#The random forest model performed the best between the three.

best_tree <- select_best(multi_tune_res)
best_tree_model <- finalize_workflow(forest_wf, best_forest_model)
best_tree_model_fit <- fit(best_tree_model, data = pokemon_test)

prediction <- augment(best_boost_final_model_fit, new_data = pokemon_test) %>% select(type_1, .pred_class, .pred_Bug,.pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water)
accuracy(prediction, type_1, .pred_class)

prediction %>% roc_curve(type_1,.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal,.pred_Psychic, .pred_Water) %>% autoplot()

prediction %>% conf_mat(type_1, .pred_class) %>% autoplot(type = "heatmap")
```


